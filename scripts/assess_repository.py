#!/usr/bin/env python3
"""
Automated Repository Assessment Tool.

This script analyzes the codebase and generates assessment reports for categories A-O.
"""

import ast
import os
import re
import sys
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

# Configure logging
import logging

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


class RepositoryAssessor:
    def __init__(self, root_dir: str = "."):
        self.root = Path(root_dir)
        self.python_files = list(self.root.rglob("*.py"))
        self.output_dir = self.root / "docs" / "assessments"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.date_str = datetime.now().strftime("%Y-%m-%d")

    def run_all(self):
        """Run all assessments."""
        assessments = [
            ("A", "Code Structure", self.assess_code_structure),
            ("B", "Documentation", self.assess_documentation),
            ("C", "Test Coverage", self.assess_test_coverage),
            ("D", "Error Handling", self.assess_error_handling),
            ("E", "Performance", self.assess_performance),
            ("F", "Security", self.assess_security),
            ("G", "Dependencies", self.assess_dependencies),
            ("H", "CI/CD", self.assess_cicd),
            ("I", "Code Style", self.assess_code_style),
            ("J", "API Design", self.assess_api_design),
            ("K", "Data Handling", self.assess_data_handling),
            ("L", "Logging", self.assess_logging),
            ("M", "Configuration", self.assess_configuration),
            ("N", "Scalability", self.assess_scalability),
            ("O", "Maintainability", self.assess_maintainability),
        ]

        for code, name, func in assessments:
            logger.info(f"Running assessment {code}: {name}...")
            score, report = func()
            self.write_report(code, name, score, report)

        self.generate_summary()
        self.create_issues()

    def generate_summary(self):
        """Generate comprehensive summary."""
        logger.info("Generating comprehensive summary...")
        script_path = self.root / "scripts" / "generate_assessment_summary.py"
        if not script_path.exists():
            logger.warning(f"Summary script not found at {script_path}")
            return

        input_pattern = self.output_dir / f"Assessment_*_Results_{self.date_str}.md"
        output_md = self.output_dir / "Comprehensive_Assessment.md"
        output_json = self.output_dir / "assessment_summary.json"

        cmd = [
            sys.executable,
            str(script_path),
            "--input", str(input_pattern),
            "--output", str(output_md),
            "--json-output", str(output_json)
        ]

        try:
            subprocess.run(cmd, check=True)
            logger.info("Summary generated successfully.")
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to generate summary: {e}")

    def create_issues(self):
        """Create issues from assessment findings."""
        logger.info("Checking for issues to create...")
        script_path = self.root / "scripts" / "create_issues_from_assessment.py"
        if not script_path.exists():
            logger.warning(f"Issue creation script not found at {script_path}")
            return

        summary_json = self.output_dir / "assessment_summary.json"

        cmd = [
            sys.executable,
            str(script_path),
            "--input", str(summary_json),
            "--dry-run" # Default to dry-run for safety
        ]

        try:
            subprocess.run(cmd, check=True)
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to run issue creation script: {e}")

    def write_report(self, category_code: str, category_name: str, score: float, content: str):
        """Write assessment report to file."""
        safe_name = category_name.replace(" ", "_").replace("/", "-")
        filename = f"Assessment_{category_code}_{safe_name}.md"
        filepath = self.output_dir / filename

        # Also create the timestamped result file expected by the summary script
        result_filename = f"Assessment_{category_code}_Results_{self.date_str}.md"
        result_filepath = self.output_dir / result_filename

        full_content = f"""# Assessment {category_code}: {category_name}

**Date**: {self.date_str}
**Score**: {score:.1f}/10

## Summary
{content}

---
*Generated by Jules Assessment Tool*
"""
        with open(filepath, "w") as f:
            f.write(full_content)

        with open(result_filepath, "w") as f:
            f.write(full_content)

        logger.info(f"Generated {filepath} and {result_filepath}")

    def _count_lines(self, files: List[Path]) -> int:
        count = 0
        for f in files:
            try:
                count += sum(1 for _ in open(f, encoding="utf-8", errors="ignore"))
            except Exception:
                pass
        return count

    def assess_code_structure(self) -> Tuple[float, str]:
        """Assess Category A: Code Structure."""
        score = 8.0
        details = []

        # Check directory structure
        expected_dirs = ["engines", "shared", "tools", "docs", "scripts", "tests"]
        present_dirs = [d for d in expected_dirs if (self.root / d).is_dir()]
        missing_dirs = set(expected_dirs) - set(present_dirs)

        if not missing_dirs:
            details.append("- Directory structure follows expected pattern.")
        else:
            score -= 1.0 * len(missing_dirs)
            details.append(f"- Missing directories: {', '.join(missing_dirs)}")

        # Check for modularity (heuristic: file count vs directory count)
        dirs_with_py = set(p.parent for p in self.python_files)
        if len(dirs_with_py) > 5:
            details.append(f"- Code is distributed across {len(dirs_with_py)} directories, indicating modularity.")
        else:
            score -= 1.0
            details.append("- Code seems flat (few directories).")

        # Check for __init__.py files
        missing_init = []
        for d in dirs_with_py:
            if not (d / "__init__.py").exists():
                missing_init.append(str(d))

        if missing_init:
            score -= 0.5 * min(len(missing_init), 4)
            details.append(f"- Missing `__init__.py` in {len(missing_init)} directories (e.g., {missing_init[:3]}).")

        return max(0, score), "\n".join(details)

    def assess_documentation(self) -> Tuple[float, str]:
        """Assess Category B: Documentation."""
        score = 7.0
        details = []

        # Check README
        if (self.root / "README.md").exists():
            details.append("- README.md exists.")
        else:
            score -= 2.0
            details.append("- README.md is missing.")

        # Check docstrings
        docstring_count = 0
        func_count = 0
        for py_file in self.python_files:
            try:
                with open(py_file, "r", encoding="utf-8") as f:
                    tree = ast.parse(f.read())
                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                        func_count += 1
                        if ast.get_docstring(node):
                            docstring_count += 1
            except Exception:
                pass

        if func_count > 0:
            coverage = docstring_count / func_count
            details.append(f"- Docstring coverage: {coverage:.1%}")
            if coverage < 0.2:
                score -= 3.0
            elif coverage < 0.5:
                score -= 1.0

        return max(0, score), "\n".join(details)

    def assess_test_coverage(self) -> Tuple[float, str]:
        """Assess Category C: Test Coverage."""
        score = 6.0
        details = []

        tests_dir = self.root / "tests"
        if not tests_dir.exists():
            return 0.0, "No `tests/` directory found."

        test_files = list(tests_dir.rglob("test_*.py"))
        details.append(f"- Found {len(test_files)} test files.")

        if len(test_files) < 5:
            score -= 3.0
            details.append("- Test suite seems sparse.")

        # Heuristic: Ratio of test files to source files
        ratio = len(test_files) / max(1, len(self.python_files))
        details.append(f"- Test file ratio: {ratio:.2f}")

        if ratio < 0.1:
            score -= 2.0
            details.append("- Low ratio of tests to source code.")

        return max(0, score), "\n".join(details)

    def assess_error_handling(self) -> Tuple[float, str]:
        """Assess Category D: Error Handling."""
        score = 7.0
        details = []

        try_except_count = 0
        bare_except_count = 0
        raise_count = 0

        for py_file in self.python_files:
            try:
                content = py_file.read_text(encoding="utf-8", errors="ignore")
                try_except_count += content.count("try:")
                bare_except_count += content.count("except:") - content.count("except ")
                raise_count += content.count("raise ")
            except Exception:
                pass

        details.append(f"- Found {try_except_count} try-except blocks.")
        details.append(f"- Found {raise_count} explicit error raises.")

        if bare_except_count > 0:
            score -= 0.1 * min(bare_except_count, 10)
            details.append(f"- Found {bare_except_count} potential bare except clauses (discouraged).")

        return max(0, score), "\n".join(details)

    def assess_performance(self) -> Tuple[float, str]:
        """Assess Category E: Performance."""
        score = 7.0
        details = []

        # Check for profiling tools
        if list(self.root.rglob("*profile*.py")):
            details.append("- Profiling scripts detected.")
        else:
            details.append("- No explicit profiling scripts found.")

        # Check for heavy loops or known slow patterns (very basic)
        return max(0, score), "\n".join(details)

    def assess_security(self) -> Tuple[float, str]:
        """Assess Category F: Security."""
        score = 8.0
        details = []

        # Check for sensitive keywords
        sensitive_keywords = ["api_key", "password", "secret", "token"]
        findings = 0
        for py_file in self.python_files:
            if "test" in str(py_file): continue
            try:
                content = py_file.read_text(encoding="utf-8", errors="ignore")
                for kw in sensitive_keywords:
                    if kw in content.lower() and "=" in content:
                        # Very naive check
                        findings += 1
            except Exception:
                pass

        if findings > 0:
            score -= 0.5 * min(findings, 4)
            details.append(f"- Found {findings} potential hardcoded secrets (keywords).")
        else:
            details.append("- No obvious hardcoded secrets found.")

        return max(0, score), "\n".join(details)

    def assess_dependencies(self) -> Tuple[float, str]:
        """Assess Category G: Dependencies."""
        score = 8.0
        details = []

        if (self.root / "pyproject.toml").exists():
            details.append("- pyproject.toml exists.")
        elif (self.root / "requirements.txt").exists():
            details.append("- requirements.txt exists.")
        else:
            score -= 2.0
            details.append("- No dependency definition found.")

        return max(0, score), "\n".join(details)

    def assess_cicd(self) -> Tuple[float, str]:
        """Assess Category H: CI/CD."""
        score = 8.0
        details = []

        workflows_dir = self.root / ".github" / "workflows"
        if workflows_dir.exists() and list(workflows_dir.glob("*.yml")):
            details.append(f"- Found {len(list(workflows_dir.glob('*.yml')))} GitHub Actions workflows.")
        else:
            score -= 3.0
            details.append("- No GitHub Actions workflows found.")

        return max(0, score), "\n".join(details)

    def assess_code_style(self) -> Tuple[float, str]:
        """Assess Category I: Code Style."""
        score = 8.0
        details = []

        # Check for configuration
        if (self.root / "pyproject.toml").exists():
            content = (self.root / "pyproject.toml").read_text()
            if "tool.ruff" in content:
                details.append("- Ruff configuration found.")
            if "tool.black" in content:
                details.append("- Black configuration found.")

        return max(0, score), "\n".join(details)

    def assess_api_design(self) -> Tuple[float, str]:
        """Assess Category J: API Design."""
        score = 7.0
        details = []

        api_dir = self.root / "api"
        if api_dir.exists():
            details.append("- API directory exists.")
            # Check for FastAPI
            content = ""
            for f in api_dir.rglob("*.py"):
                try: content += f.read_text()
                except: pass

            if "FastAPI" in content:
                details.append("- FastAPI usage detected.")
            else:
                details.append("- API framework unclear.")
        else:
            score -= 1.0 # Not critical if not an API project
            details.append("- No explicit `api/` directory.")

        return max(0, score), "\n".join(details)

    def assess_data_handling(self) -> Tuple[float, str]:
        """Assess Category K: Data Handling."""
        score = 7.0
        details = []

        # Check for data directories
        if (self.root / "data").exists() or (self.root / "output").exists():
            details.append("- Data/Output directories exist.")

        return max(0, score), "\n".join(details)

    def assess_logging(self) -> Tuple[float, str]:
        """Assess Category L: Logging."""
        score = 7.0
        details = []

        logging_usage = 0
        structlog_usage = 0
        for py_file in self.python_files:
            try:
                content = py_file.read_text(encoding="utf-8", errors="ignore")
                if "logging." in content:
                    logging_usage += 1
                if "structlog" in content:
                    structlog_usage += 1
            except: pass

        details.append(f"- Standard logging used in {logging_usage} files.")
        details.append(f"- Structlog used in {structlog_usage} files.")

        if logging_usage + structlog_usage == 0:
            score -= 2.0
            details.append("- Minimal logging detected.")

        return max(0, score), "\n".join(details)

    def assess_configuration(self) -> Tuple[float, str]:
        """Assess Category M: Configuration."""
        score = 8.0
        details = []

        config_files = list(self.root.glob("*.toml")) + list(self.root.glob("*.yaml")) + list(self.root.glob("*.json"))
        details.append(f"- Found {len(config_files)} configuration files in root.")

        return max(0, score), "\n".join(details)

    def assess_scalability(self) -> Tuple[float, str]:
        """Assess Category N: Scalability."""
        score = 7.0
        details = []

        # Hard to assess statically. Check for async.
        async_count = 0
        for py_file in self.python_files:
            try:
                if "async def" in py_file.read_text(encoding="utf-8", errors="ignore"):
                    async_count += 1
            except: pass

        details.append(f"- Found {async_count} files with async functions.")
        return max(0, score), "\n".join(details)

    def assess_maintainability(self) -> Tuple[float, str]:
        """Assess Category O: Maintainability."""
        score = 7.0
        details = []

        # Check for TODOs
        todo_count = 0
        for py_file in self.python_files:
            try:
                content = py_file.read_text(encoding="utf-8", errors="ignore")
                todo_count += content.count("TODO")
            except: pass

        details.append(f"- Found {todo_count} TODO markers.")
        if todo_count > 50:
            score -= 1.0
        if todo_count > 100:
            score -= 1.0

        return max(0, score), "\n".join(details)

if __name__ == "__main__":
    assessor = RepositoryAssessor()
    assessor.run_all()
    print("Assessment complete.")
